import os
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from pydantic import BaseModel

from vector_search import get_case_snippets
from gpt_refiner import refine_case_snippets
from query_refiner import refine_query

# âœ… import your anatomy pipeline
from anatomy_gpt import load_catalog_from_jsonl_file, run_pipeline

from openai import OpenAI

app = FastAPI()

# â”€â”€ CORS (adjust in production) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Global singletons (loaded once)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATALOG: List[Dict[str, Any]] = []
OPENAI_CLIENT: Optional[OpenAI] = None

#from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent

APPROACH_CATALOG_PATHS = [
    os.getenv(
        "UPPER_APPROACH_CATALOG_PATH",
        str(BASE_DIR / "data" / "upper_extremity" / "approaches" / "upper_extremity_approaches.jsonl"),
    ),
    os.getenv(
        "LOWER_APPROACH_CATALOG_PATH",
        str(BASE_DIR / "data" / "lower_extremity" / "approaches" / "lower_extremity_approaches.jsonl"),
    ),
]

@app.on_event("startup")
def _startup():
    global CATALOG, OPENAI_CLIENT

    # âœ… Load both catalogs
    CATALOG = []
    for p in APPROACH_CATALOG_PATHS:
        CATALOG.extend(load_catalog_from_jsonl_file(p))

    print(f"âœ… Loaded approach catalog: {len(CATALOG)} items")
    for p in APPROACH_CATALOG_PATHS:
        print(f"   - {p}")

    # âœ… Reuse one OpenAI client
    OPENAI_CLIENT = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        project=os.getenv("OPENAI_PROJECT_ID"),  # optional
    )
    print("âœ… OpenAI client initialized")


# â”€â”€ Request Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CasePrepRequest(BaseModel):
    prompt: str


# â”€â”€ Root Sanity Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/")
def read_root():
    return {"message": "SnapOrtho CasePrep API is live."}


# â”€â”€ Case Prep Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/case-prep")
async def case_prep(request: CasePrepRequest):
    prompt = (request.prompt or "").strip()
    if not prompt:
        return {"pimpQuestions": [], "otherUsefulFacts": ["âŒ No prompt provided"], "anatomy": None}

    # 1) Refine prompt (blocking)
    refined_prompt = await run_in_threadpool(refine_query, prompt)
    print(f"ðŸ§  Refined Prompt: {refined_prompt}")

    # 2) Pinecone search (blocking)
    snippets = await run_in_threadpool(get_case_snippets, refined_prompt)
    if not snippets:
        return {"pimpQuestions": [], "otherUsefulFacts": ["âŒ No relevant content found."], "anatomy": None}

    # 3) Kick off BOTH tasks in parallel (blocking -> threadpool)
    caseprep_task = run_in_threadpool(refine_case_snippets, prompt, snippets)

    anatomy_task = run_in_threadpool(
        run_pipeline,
        case_prompt=prompt,
        catalog=CATALOG,
        snippets=snippets,
        client=OPENAI_CLIENT,
    )

    # 4) Await both together
    caseprep_result, anatomy_result = await asyncio.gather(caseprep_task, anatomy_task)

    # 5) Merge
    return {**caseprep_result, "anatomy": anatomy_result}

# âœ… Optional: dedicated anatomy-only endpoint
@app.post("/anatomy")
async def anatomy_only(request: CasePrepRequest):
    prompt = (request.prompt or "").strip()
    if not prompt:
        return {"error": "No prompt provided"}

    if not CATALOG:
        return {"error": "Approach catalog not loaded"}

    result = await run_in_threadpool(
        run_pipeline,
        case_prompt=prompt,
        catalog=CATALOG,
        client=OPENAI_CLIENT,
    )
    return result
